\documentclass[main.tex]{subfiles}
\begin{document}
	
	
	
	
\subsubsection{Correlation Matrix for AR(2) Process}

To find the auto-correlation matrix $\textbf{R}$ of the input vector $\textbf{x}(n)=[x(n-1),\ x(n-2)]^T$ for a AR(2) proccess, the following defitions are first made. From $\textbf{R} = E\left[\textbf{x}(n)\textbf{x}^T(n)\right]$,

\begin{align*}
\textbf{R} = E \left\{ \left[
\begin{array}{cc}
x[n-1]^2 & x[n-1]x[n-2]  \\
x[n-2]x[n-1] & x[n-2]^2  \end{array}
\right]
\right\}
&=
\left[
\begin{array}{cc}
E \left\{ x[n-1]^2 \right\} & E \left\{x[n-1]x[n-2] \right\} \\
E \left\{ x[n-2]x[n-1]\right\} & E \left\{x[n-2]^2 \right\} \end{array}
\right]\\
&= 
\left[
\begin{array}{cc}
\gamma(0) & \gamma(1) \\
\gamma(1) & \gamma(0) \end{array}
\right]
\end{align*}

From here, we can consider %TODO: \cite{Garc2012a};

\begin{align}
x(n) = a_1*x(n-1) + a_2*x(n-2) + \eta(n)
\label{corr-mtx-base}
\end{align}	

Two tricks are now used to find $\gamma(0)$ and $\gamma(1)$. To find the variance, $\gamma(0)$, both sides of equation~\ref{corr-mtx-base} are squared and the expectation is taken. To find $\gamma(1)$, each side is multiplied by a $x(n-1)$ term \footnote{ A more general trick that is useful for finding the auto-correlation function for any distance can be done by multiplying eq~\ref{corr-mtx-base} by $x(n-k)$TODOcite{Cochrane1997a}. This gives the result $\gamma(k) = a_1\gamma(k-1) + a_2\gamma(k-2)$. However, it is more intutive and helpful to look at the two individual cases of $\gamma = 0,1$. }.

\begin{align*}
E\left[x^2(n)\right] &= \gamma(0) = E\left[  (a_1x(n-1) + a_2x(n-2) + \eta(n))^2  \right]\\
&= E\left[a_1^2x^2(n-1) + a_2^2x^2(n-2) + 2a_1a_2x(n-1)x(n-2)) + 2a_1x(n-1)\eta(n) + 2a_2x(n-2)\eta(n) + \eta^2(n)\right]\\
&= a_1^2\gamma(0) + a_2^2\gamma(0) + 2a_1a_2\gamma(1) + \sigma^2\\
\\
E\left[x(n)x(n-1)\right] &= \gamma(1) = E\left[  a_1x(n-1)x(n-1) + a_2x(n-2)x(n-1) + \eta(n)x(n-1)  \right]\\
&= a_1\gamma(0) + a_2\gamma(1)
\end{align*}


Substituting $a_1 = 0.1, a_2 = 0.8, \sigma_n^2 = 0.25$, the following equations are solved,
\begin{align*}
\gamma(0) = 0.1^2\gamma(0) + 0.8^2 + 2(0.1)(0.8)\gamma(1) + 0.25\\
\gamma(1) = 0.1\gamma(0) + 0.8\gamma(1)
\end{align*}

Which results in $\gamma(0) = 25/27$ and $\gamma(0) = 25/54$, such that

\begin{equation}
\textbf{R} = \left[
\begin{array}{cc}
\gamma(0) & \gamma(1) \\[6pt]
\gamma(1) &  \gamma(0) \end{array}
\right] = \left[
\begin{array}{cc}
\frac{25}{27} & \frac{25}{54} \\[6pt]
\frac{25}{54} &  \frac{25}{27} \end{array}
\right]
\end{equation}



\subsubsection{Leaky LMS}

\begin{align*}
\textbf{w}(n+1) &= \textbf{w}(n) - \mu\frac{\partial J_2}{\partial w}\\
\textbf{w}(n+1) &= \textbf{w}(n) - \mu\left(\frac{\partial}{\partial w}\left[\frac{1}{2}e^2(n)\right] + \frac{\partial}{\partial w}\left[\frac{1}{2}\gamma||\textbf{w}(n)||^2_2\right]\right)\\
\textbf{w}(n+1) &= \textbf{w}(n) - \mu\left(\left[-e(n)\textbf{x}(n)\right] + \left[\gamma\textbf{w}(n)\right]\right)\\
\textbf{w}(n+1) &= \textbf{w}(n) - \mu\gamma\textbf{w}(n) + \mu e(n)\textbf{x}(n)\\
\textbf{w}(n+1) &= (1-\mu\gamma)\textbf{w}(n) + \mu e(n)\textbf{x}(n)
\end{align*}




\subsubsection{Normalised LMS (NLMS)}


\begin{align*}
e_p(n) &= d(n) - \textbf{x}^T\textbf{w}(n+1)\\
e_p(n) &= d(n) - \textbf{x}^T(n)\left[\textbf{w}(n)+\mu e_p(n)\textbf{x}(n)\right]\\
e_p(n) &= d(n) - \textbf{x}^T(n)\textbf{w}(n) - \mu e_p||\textbf{x}(n)||^2\\
e_p(1+\mu||\textbf{x}(n)||^2)) &= d(n)-\textbf{x}^T(n)\textbf{w}(n) = e(n)\\
e_p(n) &= \frac{e(n)}{1+\mu||\textbf{x}(n)||^2} = e(n)\frac{1+\mu||\textbf{x}(n)||^2-\mu||\textbf{x}(n)||^2}{1+\mu||\textbf{x}(n)||^2}\\
e_p(n) &= e(n)\left[1 - \mu\frac{||\textbf{x}(n)||^2}{1+\mu||\textbf{x}(n)||^2}\right]\\
\end{align*}

The update function with the \textit{a posteriori} error can now be shown to be equivalent to the NLMS.

\begin{align*}
\Delta \textbf{w}(n) &= \mu e(n)\left[1-\mu\frac{||\textbf{x}(n)||^2}{1+\mu||\textbf{x}(n)||^2}\right]\textbf{x}(n)\\
&= e(n)\left[\mu-\mu^2\frac{||\textbf{x}(n)||^2}{\mu\left(\frac{1}{\mu}+||\textbf{x}(n)||^2\right)}\right]\textbf{x}(n)\\
&= e(n)\left[\frac{1+\mu||\textbf{x}(n)||^2}{\frac{1}{\mu}+||\textbf{x}(n)||^2}-\frac{\mu||\textbf{x}(n)||^2}{\frac{1}{\mu}+||\textbf{x}(n)||^2}\right]\textbf{x}(n)\\
&= e(n)\left[\frac{1}{\frac{1}{\mu}+||\textbf{x}(n)||^2}\right]\textbf{x}(n)\\
\text{Thus,\ \ } \textbf{w}(n+1) &= \textbf{w}(n) + \frac{1}{\frac{1}{\mu}+||\textbf{x}(n)||^2}e(n)\textbf{x}(n)
\end{align*}

Which is the NLMS algorithm, with $\beta$ = 1 and $\epsilon$ = $\frac{1}{\mu}$.







\subsubsection{Clarke Voltage Frequency from Filter Coefficients}


The frequency of the Clarke Voltage can be extracted from the filter coefficients of the CLMS or the ACLMS. Two seperate situations are investigated: the strictly linear (Equation~\ref{eq-4-1-d-first}) and the widely linear (Equation~\ref{eq-4-1-d-second}) models.

\begin{align}
v(n+1) &= h^*(n)v(n) \label{eq-4-1-d-first}\\
v(n+1) &= h^*(n)v(n) + g^*(n)v^*(n) \label{eq-4-1-d-second}
\end{align}

\paragraph{For the strictly linear model,} equation~\ref{eq-4-1-c-bal} is used to find expressions for $v(n+1)$ and $v(n)$. These are substituted into equation~\ref{eq-4-1-d-first} to give

\begin{align*}
\sqrt{\frac{3}{2}} V e^{j(2 \pi \frac{f_0}{f_s}(n+1) + \phi)} &= h^*(n)\sqrt{\frac{3}{2}} V e^{j(2 \pi \frac{f_0}{f_s}n + \phi)}\\
e^{j(2 \pi \frac{f_0}{f_s}n + \phi)}e^{j(2 \pi \frac{f_0}{f_s})} &= h^*(n) e^{j(2 \pi \frac{f_0}{f_s}n + \phi)}\\
e^{j(2 \pi \frac{f_0}{f_s})} &= h^*(n) \\
arg(e^{j(2 \pi \frac{f_0}{f_s})}) &= arg(h^*(n)) \\
2 \pi \frac{f_0}{f_s} &= arctan\left(\frac{\Im(h^*(x))}{\Re(h^*(x))}\right) \\
f_0 &= - \frac{f_s}{2\pi} arctan\left(\frac{\Im(h(x))}{\Re(h(x))}\right) \\
\end{align*}

\paragraph{For the widely linear model,} the equations \ref{eq-4-1-c-unbal} and \ref{eq-4-1-d-second} are used to gather terms for $v(n+1)$ and $\hat{v}(n+1)$. The coefficients for both of these terms are equated to extrapolate the value of $f_0$ %TODO \cite{Xia2012} 

\begin{align*}
v(n+1) &= A(n+1)e^{j2\pi\frac{f_0}{f_s}}e^{j(2\pi  \frac{f_0}{f_s}n+\phi)} + B(n+1)e^{-j2\pi\frac{f_0}{f_s}}e^{-j(2\pi  \frac{f_0}{f_s}n+\phi)}\\
\hat{v}(n+1) &= h^*(n)\left(A(n)e^{j(2\pi\frac{f_0}{f_s}n+\phi)} + B(n)e^{-j(2\pi\frac{f_0}{f_s}n+\phi)}\right) + g^*(n)\left(A^*(n)e^{-j(2\pi\frac{f_0}{f_s}n+\phi)} + B^*(n)e^{j(2\pi\frac{f_0}{f_s}n+\phi)}\right)\\
&= \left[h^*(n)A(n) + g^*(n)B^*(n)\right] e^{j(2\pi\frac{f_0}{f_s}n+\phi)} + \left[h^*(n)B(n) + g^*(n)A^*(n)\right] e^{-j(2\pi\frac{f_0}{f_s}n+\phi)}
\end{align*}

At steady state, we can make two assumptions; Firstly, $A(n) \simeq A(n+1)$ and $B(n) \simeq B(n+1)$. Secondly, $v(n+1) \simeq \hat{v}(n+1)$. With these two assumptions, we can equate the first and second right hand side (RHS) terms,to find the two equations for the term $e^{j2\pi\frac{f_0}{f_s}}$;

\begin{align*}
e^{j2\pi\frac{f_0}{f_s}} &= \frac{A(n)h^*(n) + B^*(n)g^*(n)}{A(n+1)} \simeq \frac{A(n)h^*(n) + B^*(n)g^*(n)}{A(n)} \\
e^{-j2\pi\frac{f_0}{f_s}} &= \frac{A^*(n)g^*(n) + B(n)h^*(n)}{B(n+1)} \simeq \frac{A^*(n)g^*(n) + B(n)h^*(n)}{B(n)}\\
\left[e^{-j2\pi\frac{f_0}{f_s}}\right]^* &= e^{j2\pi\frac{f_0}{f_s}} =  \frac{A(n)g(n) + B^*(n)h(n)}{B^*(n)}
\end{align*}

Thus, we get

\begin{align*}
e^{j2\pi\frac{f_0}{f_s}} &= h^*(n) + \frac{B^*(n)}{A(n)}g^*(n)\\
e^{j2\pi\frac{f_0}{f_s}} &= h(n) + \frac{A(n)}{B^*(n)}g(n)
\end{align*}

It can be shown that $A(n)$ will always be real, and that $B(n)$ is complex. Therefore, $B^*(n)/A(n) = \left(B(n)/A(n)\right)^*$. If we consider $a(n) = (B(n)/A(n))^*$, and equate the two equations above,

\begin{align*}
h^*(n) + a(n)g^*(n) = h(n) + \frac{1}{a(n)}g(n)\\
a^2(n)\left[g^*(n)\right] + a(n)\left[h^*(n) - h(n)\right] + \left[-g(n)\right] &= 0\\
a^2(n)\left[g^*(n)\right] + a(n)\left[2j\Im(h^*(n))\right] + \left[-g(n)\right] &= 0
\end{align*}

Solving for $a(n)$,

\begin{align*}
a_{1,2}(n) &= \frac{-2j\Im(h^*(n)) \pm \sqrt{ (-2j\Im(h^*(n)))^2 -4(g^*(n) * g(n)) } }{2g^*(n)}\\
a_{1,2}(n) &= \frac{-j\Im(h^*(n)) \pm \sqrt{ -\Im^2(h^*(n)) -|g^*(n)|^2 } }{g^*(n)}\\
a_{1,2}(n) &= \frac{-j\Im(h^*(n)) \pm j\sqrt{ \Im^2(h^*(n)) + |g^*(n)|^2 } }{g^*(n)}\\
\end{align*}

Therefore, the term $e^{j2\pi\frac{f_0}{f_s}}$ is approximated by either $h^*(n) + a_1(n)g^*(n)$ or $h^*(n) + a_2(n)g^*(n)$. However, since the system frequency is much smaller than the sampling frequency, the imaginary part of $e^{j2\pi\frac{f_0}{f_s}}$ must be positive. Thus, only $a_1$ can be considered, allowing us to extrapolate a term for $f_0$.

\begin{align*}
e^{j2\pi\frac{f_0}{f_s}} &= h^*(n) + \frac{-j\Im(h^*(n)) + j\sqrt{ \Im^2(h^*(n)) + |g^*(n)|^2 } }{g^*(n)}g^*(n)\\
arg(e^{j2\pi\frac{f_0}{f_s}}) &= arg\left(h^*(n) + -j\Im(h^*(n)) + j\sqrt{ \Im^2(h^*(n)) + |g^*(n)|^2 } \right)\\
2\pi\frac{f_0}{f_s} &= arctan(\frac{\Im(h^*(n)) - \Im(h^*(n)))+\sqrt{ \Im^2(h^*(n)) + |g^*(n)|^2 }}{\Re(h^*(n))})\\
f_0 &= \frac{f_s}{2\pi}arctan(\frac{\sqrt{ \Im^2(h^*(n)) + |g^*(n)|^2 }}{\Re(h^*(n))}) = \frac{f_s}{2\pi}arctan(\frac{\sqrt{ \Im^2(h(n)) + |g(n)|^2 }}{\Re(h(n))})
\end{align*}



\end{document}