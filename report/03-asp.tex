\documentclass[main.tex]{subfiles}
\begin{document}
\section{Adaptive Signal Processing}


\subsection{The Least Mean Square (LMS) Algorithm}

When studying non-linear signals, adaptive filters that approximate the Wiener solutions are considered. The primary focus here will be on the Least Mean Squared (LMS) algorithm, which recursively adjusts the weights following the gradient method of optimisation. Essentially, the weights are updated according to the following equation;

\begin{align*}
w(n+1) &= w(n) + \mu \frac{\partial}{\partial w}\left( \frac{1}{2}e^2(n) \right),\ \ \ \ \ \ n=0,1,...  \\
&= w(n) + \mu e(n) x(n) \numberthis \label{equation-lms}
\end{align*}

where $e(n) = x(n) - \hat{x}(n)$. In what followed, we look at the LMS algorithm under a number of circumstances and try to ensure that conversion is always possible and reasonably fast. % TODO: quanitfy this



\subsubsection{Correlation Matrix for AR(2) Process}

To find the auto-correlation matrix $\textbf{R}$ of the input vector $\textbf{x}(n)=[x(n-1),\ x(n-2)]^T$ for a AR(2) proccess, the following defitions are first made. From $\textbf{R} = E\left[\textbf{x}(n)\textbf{x}^T(n)\right]$,

\begin{align*}
\textbf{R} = E \left\{ \left[
\begin{array}{cc}
x[n-1]^2 & x[n-1]x[n-2]  \\
x[n-2]x[n-1] & x[n-2]^2  \end{array}
\right]
\right\}
&=
\left[
\begin{array}{cc}
E \left\{ x[n-1]^2 \right\} & E \left\{x[n-1]x[n-2] \right\} \\
E \left\{ x[n-2]x[n-1]\right\} & E \left\{x[n-2]^2 \right\} \end{array}
\right]\\
&= 
\left[
\begin{array}{cc}
\gamma(0) & \gamma(1) \\
\gamma(1) & \gamma(0) \end{array}
\right]
\end{align*}

From here, we can consider\cite{Garc2012a};

\begin{align}
x(n) = a_1*x(n-1) + a_2*x(n-2) + \eta(n)
\label{corr-mtx-base}
\end{align}	

Two tricks are now used to find $\gamma(0)$ and $\gamma(1)$. To find the variance, $\gamma(0)$, both sides of equation~\ref{corr-mtx-base} are squared and the expectation is taken. To find $\gamma(1)$, each side is multiplied by a $x(n-1)$ term \footnote{ A more general trick that is useful for finding the auto-correlation of any two values can be done by multiplying eq~\ref{corr-mtx-base} by $x(n-k)$. This gives the result $\gamma(k) = a_1\gamma(k-1) + a_2\gamma(k-2)$. However, it is more intutive and helpful to look at the two individual cases of $\gamma = 0,1$. }.

\begin{align*}
E\left[x^2(n)\right] = \gamma(0) &= E\left[  (a_1x(n-1) + a_2x(n-2) + \eta(n))^2  \right]\\
&= E\left[a_1^2x^2(n-1) + a_2^2x^2(n-2) + 2a_1a_2x(n-1)x(n-2)) + \sigma^2\right]\\
&= a_1^2\gamma(0) + a_2^2\gamma(0) + 2a_1a_2\gamma(1) + \sigma^2\\
\\
E\left[x(n)x(n-1)\right] = \gamma(1) &= E\left[  a_1x(n-1)x(n-1) + a_2x(n-2)x(n-1) + \eta(n)x(n-1)  \right]\\
&= a_1\gamma(0) + a_2\gamma(1)
\end{align*}


Substituting $a_1 = 0.1, a_2 = 0.8, \sigma_n^2 = 0.25$, the following equations are solved,
\begin{align*}
\gamma(0) = 0.1^2\gamma(0) + 0.8^2 + 2(0.1)(0.8)\gamma(1) + 0.25\\
\gamma(1) = 0.1\gamma(0) + 0.8\gamma(1)
\end{align*}
 
Which results in $\gamma(0) = 25/27$ and $\gamma(0) = 25/54$, such that

\begin{equation}
\textbf{R} = \left[
\begin{array}{cc}
\gamma(0) & \gamma(1) \\[6pt]
\gamma(1) &  \gamma(0) \end{array}
\right] = \left[
\begin{array}{cc}
\frac{25}{27} & \frac{25}{54} \\[6pt]
\frac{25}{54} &  \frac{25}{27} \end{array}
\right]
\end{equation}










\subsubsection{Implementation of LMS Adaptive Predictor}

Over 1000 samples, the performance of the LMS-algorithm for parameter prediction can be calculate by looking at the squared prediction errror. To do this, a $\mu$ value of $0.01$ and $0.05$ is considered. Both a single run of the algorithm, as well as an averaged error over 100 iterations of the algorithm, are shown.

(graphs)

As expected, the higher value of $\mu$ converges faster (as it's learning parameters change more rapidly)\footnote{Faster changes are not always good - a higher value of $\mu$ sometimes leads to instability}. It's interesting to note that for both values of $\mu$, the error reaches values near $-6dB$. This is from the fact that our noise is from $var(\sigma_n^2 = 0.25)$, and $10*log_{10}(0.25) = -6.021$.


\subsubsection{Misadjustment Rate}


As seen in figure TODO, it can be seen that the error term tends to fluctuate around the minimum sqare error ($\sigma_n^2$). However, there is an excess to this error, which is dubbed the excess mean square error (EMSE), defined as $EMSE = MSE - \sigma_2^2$. The \textbf{misadjustment rate} is simply the ratio between the EMSE and the minimum mean square error.

\begin{equation*}
\mathcal{M} = \frac{EMSE}{\sigma_n^2}
\end{equation*}

Looking over the estimated and the caluclated values for the misadjustment rate, it can be seen that they lie within the same order of magnitude. Furthermore, as the value of $\mu$ increases, as does the misadjustment rate. This comes from the fact that steady state fluctuations are more likely due to a larger step.

\begin{table}[h]
	\centering
	\begin{tabular}{c c}
		$\mathcal{M}$ & $\hat{\mathcal{M}}_{LMS} $ \\
		0.008395      & 0.00926                   \\
		0.043028      & 0.04630                  
	\end{tabular}
\end{table}










\subsubsection{Steady State Filter Coefficients}

Over 100 independant trials of the LMS algorithm, the mean values of the estimated filter coefficients $\hat{a}_1$ and $\hat{a}_2$ are plotted. As shown, the coefficients approach the actual values with only TODO\% error. 

graph










\subsubsection{Leaky LMS}

The cost function used in the standard LMS algorithm does not always converge. TODO: When does it not. A work-around in these situations is to look at changing the error function to include a leakage term, allowing for convergance. 

In this situation, the error function that is to be minimised is change to include an additional term; $J_2(n) = \frac{1}{2}(e^2(n) + \gamma||w(n)||^2_2)$. From here, it is trivial to prove that the weights update according to $\textbf{w}(n+1) = (1-\mu \gamma)\textbf{w}(n) + \mu e(n)\textbf{x}(n)$.

add the proof.








\subsubsection{Implementation}

graph

Thought the values to approach the original filter parameters $[0.1 0.8]$, there is a considerably larger margin to the original LMS filter. This can be explained by the addition of the leak, which continually reduces the strength of the weights, even in steady state. In fact, it can be shown that the convergence of the weights is related to a corrupted version of the correlation matrix\cite{Kamenetsky2004}:

\begin{align*}
\lim_{k \to \inf}E(\textbf{w}(k)) = (\textbf{R} + \gamma \textbf{I})^{-1}\textbf{p}
\end{align*}

where $\textbf{I}$ is the identity matrix and $\textbf{p}$ the cross correlation vector. The additional corruption of the auto-correlation matrix allows it to be inverted, but yields biased results.

% TODO: define cross correlation


















\subsection{Adaptive Step Sizes}

In the above section, a constant step size $\mu$ was selected. It should be noted that a higher step size will converge to the correct answer sooner, but can overshoot and cause instability when set too high. A lower one will converge more slowly, but will tend to be more stable once converegence has been achieved. As such, a desirable trait in the step size is to allow them to be adaptive; i.e. higher before convergence, and lower after convergence. 

To this purpose, three algorithms are investiaged. Each algorithm adaptively updates the step, $\mu$, according to the following equations:

%TODO: eqns


	
\subsubsection{Implementation of Adaptive Step Sizes}






\subsubsection{Normalised LMS (NLMS)}

As the weight update for the LMS algorithm ($\Delta w$) is proportional to the magnitude of the input vector $\textbf{x}(n)$, it's not uncommon to normalise the step size. This is done by dividing $\Delta w$ by the magnitude of the input vector, summed with a regularisation factor $\epsilon$ to ensure stability of the algorithm even when the input is 0.

\begin{equation*}
\textbf{w}(n+1) = \textbf{w}(n) + \frac{\beta}{\epsilon + ||\textbf{x}(n)||^2}e(n)\textbf{x}(n)
\end{equation*}

By starting with an updated function that relied on the \textit{a posteriori} error, $e_p(n) = d(n) - \textbf{x}^T(n)\textbf{w}(n+1)$, it can be shown that this is an implementation of the NLMS algorithm.

proof

with something = 1 and something = $\frac{1}{\mu}$.



\subsubsection{Generalised Normalised Gradient Descent Algorithm (GNGD)}












\end{document}
